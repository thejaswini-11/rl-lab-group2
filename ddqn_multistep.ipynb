{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN vs Double DQN , DQN vs Multistep DQN and DQN vs Dueling Networks DQN\n",
    "\n",
    "This notebook implements and compares:\n",
    "- Vanilla DQN\n",
    "- Double DQN \n",
    "- Multi-step DQN\n",
    "- Dueling Networks DQN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (0.23.0)\n",
      "Requirement already satisfied: torchaudio in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: numpy in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: gymnasium==0.29.1 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from gymnasium==0.29.1) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from gymnasium==0.29.1) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from gymnasium==0.29.1) (4.14.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from gymnasium==0.29.1) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from gymnasium==0.29.1) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gymnasium==0.29.1) (3.23.0)\n",
      "Requirement already satisfied: minatar==1.0.15 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (1.0.15)\n",
      "Requirement already satisfied: cycler>=0.10.0 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from minatar==1.0.15) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from minatar==1.0.15) (1.4.7)\n",
      "Requirement already satisfied: matplotlib>=3.0.3 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from minatar==1.0.15) (3.9.4)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from minatar==1.0.15) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from minatar==1.0.15) (2.3.3)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from minatar==1.0.15) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from minatar==1.0.15) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2018.9 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from minatar==1.0.15) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from minatar==1.0.15) (1.13.1)\n",
      "Requirement already satisfied: seaborn>=0.9.0 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from minatar==1.0.15) (0.13.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from minatar==1.0.15) (1.17.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from matplotlib>=3.0.3->minatar==1.0.15) (1.3.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from matplotlib>=3.0.3->minatar==1.0.15) (4.60.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from matplotlib>=3.0.3->minatar==1.0.15) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from matplotlib>=3.0.3->minatar==1.0.15) (11.3.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from matplotlib>=3.0.3->minatar==1.0.15) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib>=3.0.3->minatar==1.0.15) (3.23.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from pandas>=0.24.2->minatar==1.0.15) (2025.2)\n",
      "Requirement already satisfied: matplotlib in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (3.9.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.23.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: pandas in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/thejaswini/.virtualenvs/rl_lab/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install gymnasium==0.29.1\n",
    "!pip install minatar==1.0.15\n",
    "!pip install matplotlib\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_shape: torch.Size, num_actions: int):\n",
    "        \"\"\"\n",
    "        Initialize the DQN network.\n",
    "        \n",
    "        :param obs_shape: Shape of the observation space\n",
    "        :param num_actions: Number of actions\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(obs_shape[-1], 16, stride=1, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(16, 32, stride=1, kernel_size=3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 128)\n",
    "        self.fc2 = nn.Linear(128, num_actions)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Dueling networks extention\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, obs_shape: torch.Size, num_actions: int):\n",
    "      \n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(obs_shape[-1], 16, stride=1, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(16, 32, stride=1, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 128)\n",
    "       \n",
    "        # New dueling heads\n",
    "        self.value_fc = nn.Linear(128, 1)          # V(s), value stream\n",
    "        self.advantage_fc = nn.Linear(128, num_actions)  # A(s, a) ,  advantage stream\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "       \n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "\n",
    "        x = torch.flatten(x, 1) # flatten the intermediate result such that it can serve as input for the first linear layer\n",
    "\n",
    "        # Final layer consists of 128 \"rectifier\" units meaning a ReLU activation\n",
    "        x = self.relu(self.fc1(x))\n",
    "        # Dueling streams\n",
    "        value = self.value_fc(x)              # shape: (batch, 1)\n",
    "        advantage = self.advantage_fc(x)      # shape: (batch, num_actions)\n",
    "        \n",
    "        # aggregate  value + advantage to get Q(s,a) \n",
    "        q_values = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        return q_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon-Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q: nn.Module, num_actions: int):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "\n",
    "    :param Q: The DQN network.\n",
    "    :param num_actions: Number of actions in the environment.\n",
    "\n",
    "    :returns: A function that takes the observation as an argument and returns the greedy action.\n",
    "    \"\"\"\n",
    "    def policy_fn(obs: torch.Tensor, epsilon: float = 0.0):\n",
    "        if np.random.uniform() < epsilon:\n",
    "            return np.random.randint(0, num_actions)\n",
    "        \n",
    "        return Q(obs).argmax().detach().numpy()\n",
    "\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Decay Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_epsilon_decay(eps_start: float, eps_end: float, current_timestep: int, duration: int) -> float:\n",
    "    \"\"\"\n",
    "    Linear decay of epsilon.\n",
    "\n",
    "    :param eps_start: The initial epsilon value.\n",
    "    :param eps_end: The final epsilon value.\n",
    "    :param current_timestep: The current timestep.\n",
    "    :param duration: The duration of the schedule (in timesteps).\n",
    "\n",
    "    :returns: The current epsilon.\n",
    "    \"\"\"\n",
    "    ratio = min(1.0, current_timestep / duration)\n",
    "    return (eps_start - eps_end) * (1 - ratio) + eps_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size: int):\n",
    "        \"\"\"\n",
    "        Create the replay buffer.\n",
    "\n",
    "        :param max_size: Maximum number of transitions in the buffer.\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.max_size = max_size\n",
    "        self.position = 0\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def store(self, obs: torch.Tensor, action: torch.Tensor, reward: torch.Tensor, next_obs: torch.Tensor, terminated: torch.Tensor, n_steps: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Adds a new transition to the buffer.\n",
    "        \"\"\"\n",
    "        if len(self.data) < self.max_size:\n",
    "            self.data.append((obs, action, reward, next_obs, terminated, n_steps))\n",
    "        else:\n",
    "            self.data[self.position] = (obs, action, reward, next_obs, terminated, n_steps)\n",
    "        self.position = (self.position + 1) % self.max_size\n",
    "\n",
    "    def sample(self, batch_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample a batch of transitions uniformly and with replacement.\n",
    "        \"\"\"\n",
    "        return [torch.stack(b) for b in zip(*random.choices(self.data, k=batch_size))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-step Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nStepBuffer:\n",
    "    def __init__(self, n: int, gamma: float):\n",
    "        \"\"\"\n",
    "        Create the n-step buffer.\n",
    "\n",
    "        :param n: The number of steps to look ahead.\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.n = n\n",
    "        self.gamma = gamma\n",
    "        self.position = 0\n",
    "        self.first_n = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = []\n",
    "        self.position = 0\n",
    "        self.first_n = 0  # Should be the only attribute thats not reset anyway\n",
    "        \n",
    "    def update(self, obs: torch.Tensor, action: torch.Tensor, reward: torch.Tensor, next_obs: torch.Tensor, terminated: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Adds a new transition to the buffer.\n",
    "        \"\"\"\n",
    "        if self.first_n < self.n:\n",
    "            self.data.append((obs, action, reward, next_obs, terminated))\n",
    "        else:\n",
    "            self.data[self.position] = (obs, action, reward, next_obs, terminated)\n",
    "        self.position = (self.position + 1) % (self.n if (self.first_n < self.n and not terminated) else len(self.data))\n",
    "\n",
    "        if self.first_n < self.n and not terminated:\n",
    "            self.first_n += 1\n",
    "            return None, None, None, None, None, None\n",
    "        else:\n",
    "            R = torch.tensor(0.0)\n",
    "            for k in range(len(self.data)):\n",
    "                R = R + (self.gamma ** k) * self.data[(self.position + k) % len(self.data)][2]\n",
    "            \n",
    "            obs_t = self.data[self.position][0]\n",
    "            action_t = self.data[self.position][1]\n",
    "            next_obs_t = self.data[(self.position - 1) % len(self.data)][3]\n",
    "            terminated_t = self.data[(self.position - 1) % len(self.data)][4]\n",
    "            n_transitions = len(self.data)\n",
    "\n",
    "            if terminated:\n",
    "                if self.first_n < self.n:\n",
    "                    self.first_n = self.n\n",
    "                del self.data[(self.position - 1) % len(self.data)]\n",
    "                self.position = (self.position - 1) % len(self.data) if self.position > 0 else 0\n",
    "\n",
    "            return obs_t, action_t, R, next_obs_t, terminated_t, n_transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Functions\n",
    "\n",
    "### Vanilla DQN Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dqn(\n",
    "        q: nn.Module,\n",
    "        q_target: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        gamma: float,\n",
    "        obs: torch.Tensor,\n",
    "        act: torch.Tensor,\n",
    "        rew: torch.Tensor,\n",
    "        next_obs: torch.Tensor,\n",
    "        tm: torch.Tensor,\n",
    "        n_steps: torch.Tensor,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Update the vanilla DQN.\n",
    "    Uses the target network to both select and evaluate actions.\n",
    "    Supports n-step returns via the n_steps input.\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Vanilla DQN: Use target network to select AND evaluate\n",
    "    with torch.no_grad():\n",
    "        discount = torch.pow(torch.tensor(gamma, device=rew.device), n_steps.float())\n",
    "        td_target = rew + discount * q_target(next_obs).max(dim=1)[0] * (1 - tm.float())\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = F.mse_loss(q(obs).gather(1, act.unsqueeze(1)), td_target.unsqueeze(1))\n",
    "\n",
    "    # Backpropagate\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double DQN Update\n",
    "\n",
    "The key innovation: Use the online network to **select** the best action, but use the target network to **evaluate** it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_double_dqn(\n",
    "        q: nn.Module,\n",
    "        q_target: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        gamma: float,\n",
    "        obs: torch.Tensor,\n",
    "        act: torch.Tensor,\n",
    "        rew: torch.Tensor,\n",
    "        next_obs: torch.Tensor,\n",
    "        tm: torch.Tensor,\n",
    "        n_steps: torch.Tensor,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Update the DQN network using Double DQN.\n",
    "    Uses the online network to select actions, but target network to evaluate them.\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Double DQN: Use online network to SELECT action, target network to EVALUATE\n",
    "    with torch.no_grad():\n",
    "        # Select best action using online network\n",
    "        best_actions = q(next_obs).argmax(dim=1)\n",
    "        # Evaluate selected action using target network\n",
    "        next_q_values = q_target(next_obs).gather(1, best_actions.unsqueeze(1)).squeeze(1)\n",
    "        discount = torch.pow(torch.tensor(gamma, device=rew.device), n_steps.float())\n",
    "        td_target = rew + discount * next_q_values * (1 - tm.float())\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = F.mse_loss(q(obs).gather(1, act.unsqueeze(1)), td_target.unsqueeze(1))\n",
    "\n",
    "    # Backpropagate\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EpisodeStats = namedtuple(\"Stats\", [\"episode_lengths\", \"episode_rewards\", \"losses\"])\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self,\n",
    "            env,\n",
    "            net_type=\"vanilla\",\n",
    "            gamma=0.99,\n",
    "            lr=0.001, \n",
    "            batch_size=64,\n",
    "            eps_start=1.0,\n",
    "            eps_end=0.1,\n",
    "            schedule_duration=10_000,\n",
    "            update_freq=100,\n",
    "            maxlen=100_000,\n",
    "            n_steps=1,\n",
    "            use_double_dqn=False,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the DQN agent.\n",
    "        \n",
    "        :param env: The environment.\n",
    "        :param gamma: The discount factor.\n",
    "        :param lr: The learning rate.\n",
    "        :param batch_size: Mini batch size.\n",
    "        :param eps_start: The initial epsilon value.\n",
    "        :param eps_end: The final epsilon value.\n",
    "        :param schedule_duration: The duration of the schedule (in timesteps).\n",
    "        :param update_freq: How often to update the Q target.\n",
    "        :param max_size: Maximum number of transitions in the buffer.\n",
    "        :param use_double_dqn: If True, use Double DQN update rule\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.net_type = net_type \n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.schedule_duration = schedule_duration\n",
    "        self.update_freq = update_freq\n",
    "        self.n_steps = n_steps\n",
    "        self.n_transitions = 1\n",
    "        self.use_double_dqn = use_double_dqn\n",
    "\n",
    "        self.buffer = ReplayBuffer(maxlen)\n",
    "        self.nstep_buffer = nStepBuffer(n=n_steps, gamma=gamma)\n",
    "\n",
    "        # due to different network architecture \n",
    "        self.q = self.make_network(\n",
    "            net_type,\n",
    "            env.observation_space.shape,\n",
    "            env.action_space.n\n",
    "        )\n",
    "        \n",
    "        self.q_target = self.make_network(\n",
    "            net_type,\n",
    "            env.observation_space.shape,\n",
    "            env.action_space.n\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.q_target.load_state_dict(self.q.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q.parameters(), lr=lr)\n",
    "        self.policy = make_epsilon_greedy_policy(self.q, env.action_space.n)\n",
    "\n",
    "    # deciding which network to choose \n",
    "\n",
    "    def make_network(self, net_type, obs_shape, n_actions):\n",
    "        if net_type == \"vanilla\":\n",
    "            return DQN(obs_shape, n_actions)\n",
    "        elif net_type == \"dueling\":\n",
    "            return DuelingDQN(obs_shape, n_actions)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown net_type: {net_type}\")\n",
    "\n",
    "    def train(self, num_episodes: int) -> EpisodeStats:\n",
    "        \"\"\"\n",
    "        Train the DQN agent.\n",
    "\n",
    "        :param num_episodes: Number of episodes to train.\n",
    "        :returns: The episode statistics.\n",
    "        \"\"\"\n",
    "        stats = EpisodeStats(\n",
    "            episode_lengths=np.zeros(num_episodes),\n",
    "            episode_rewards=np.zeros(num_episodes),\n",
    "            losses=[],\n",
    "        )\n",
    "        current_timestep = 0\n",
    "        epsilon = self.eps_start\n",
    "        \n",
    "        #agent_type = \"Double DQN\" if self.use_double_dqn else \"Vanilla DQN\"\n",
    "        \n",
    "        for i_episode in range(num_episodes):\n",
    "            if (i_episode + 1) % 100 == 0:\n",
    "                print(f'Episode {i_episode + 1}/{num_episodes}  '\n",
    "                      f'Time Step: {current_timestep}  Epsilon: {epsilon:.3f}  '\n",
    "                      f'Avg Reward: {np.mean(stats.episode_rewards[max(0, i_episode-99):i_episode+1]):.2f}')\n",
    "\n",
    "            self.nstep_buffer.reset()\n",
    "            obs, _ = self.env.reset()\n",
    "            \n",
    "            for episode_time in itertools.count():\n",
    "                epsilon = linear_epsilon_decay(self.eps_start, self.eps_end, current_timestep, self.schedule_duration)\n",
    "\n",
    "                action = self.policy(torch.as_tensor(obs).unsqueeze(0).float(), epsilon=epsilon)\n",
    "                next_obs, reward, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "                stats.episode_lengths[i_episode] += 1\n",
    "                stats.episode_rewards[i_episode] += reward\n",
    "                \n",
    "                # Store in n-step buffer and get \"ready\" transition from there\n",
    "                if self.n_steps > 1:\n",
    "                    obs_t, action_t, reward_t, next_obs, terminated, n_transitions = self.nstep_buffer.update(obs, action, reward, next_obs, terminated)\n",
    "                else:\n",
    "                    obs_t = obs\n",
    "                    action_t = action\n",
    "                    reward_t = reward\n",
    "                    n_transitions = self.n_transitions\n",
    "\n",
    "                # Skip if n-step buffer is not ready yet\n",
    "                if obs_t is not None:\n",
    "\n",
    "                    self.buffer.store(\n",
    "                        torch.as_tensor(obs_t, dtype=torch.float32),\n",
    "                        torch.as_tensor(action_t),\n",
    "                        torch.as_tensor(reward_t, dtype=torch.float32),\n",
    "                        torch.as_tensor(next_obs, dtype=torch.float32),\n",
    "                        torch.as_tensor(terminated),\n",
    "                        torch.as_tensor(n_transitions)\n",
    "                    )\n",
    "\n",
    "                    if len(self.buffer) >= self.batch_size:\n",
    "                        obs_batch, act_batch, rew_batch, next_obs_batch, tm_batch, n_steps_batch = self.buffer.sample(self.batch_size)\n",
    "                        \n",
    "                        # Choose update function based on algorithm\n",
    "                        if self.use_double_dqn:\n",
    "                            loss = update_double_dqn(\n",
    "                                self.q, self.q_target, self.optimizer, self.gamma,\n",
    "                                obs_batch.float(), act_batch, rew_batch.float(),\n",
    "                                next_obs_batch.float(), tm_batch, n_steps_batch\n",
    "                            )\n",
    "                        else:\n",
    "                            loss = update_dqn(\n",
    "                                self.q, self.q_target, self.optimizer, self.gamma,\n",
    "                                obs_batch.float(), act_batch, rew_batch.float(),\n",
    "                                next_obs_batch.float(), tm_batch, n_steps_batch\n",
    "                            )\n",
    "                        \n",
    "                        stats.losses.append(loss)\n",
    "\n",
    "                    if current_timestep % self.update_freq == 0:\n",
    "                        self.q_target.load_state_dict(self.q.state_dict())\n",
    "                        \n",
    "                    current_timestep += 1\n",
    "\n",
    "                    if terminated or truncated or episode_time >= 500:\n",
    "                        # Handle pending n-step transitions\n",
    "                        while n_transitions > 1:\n",
    "                            obs_t, action_t, reward_t, next_obs, terminated, n_transitions = self.nstep_buffer.update(obs, action, reward, next_obs, terminated)\n",
    "\n",
    "                            self.buffer.store(\n",
    "                                torch.as_tensor(obs_t, dtype=torch.float32),\n",
    "                                torch.as_tensor(action_t),\n",
    "                                torch.as_tensor(reward_t, dtype=torch.float32),\n",
    "                                torch.as_tensor(next_obs, dtype=torch.float32),\n",
    "                                torch.as_tensor(terminated),\n",
    "                                torch.as_tensor(n_transitions)\n",
    "                            )\n",
    "                        break\n",
    "                        \n",
    "                    obs = next_obs\n",
    "                \n",
    "        return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on MinAtar/Breakout-v1\n",
      "Observation space: Box(False, True, (10, 10, 4), bool)\n",
      "Action space: Discrete(3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Hyperparameters\n",
    "LR = 0.00025\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_BUFFER_SIZE = 100_000\n",
    "UPDATE_FREQ = 1000\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "SCHEDULE_DURATION = 100_000\n",
    "NUM_EPISODES = 3_000\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('MinAtar/Breakout-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "print(f\"Training on {env.spec.id}\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING VANILLA DQN\n",
      "======================================================================\n",
      "\n",
      "Episode 100/3000  Time Step: 1026  Epsilon: 0.990  Avg Reward: 0.42\n",
      "Episode 200/3000  Time Step: 2010  Epsilon: 0.980  Avg Reward: 0.36\n",
      "Episode 300/3000  Time Step: 3038  Epsilon: 0.970  Avg Reward: 0.42\n",
      "Episode 400/3000  Time Step: 4216  Epsilon: 0.958  Avg Reward: 0.56\n",
      "Episode 500/3000  Time Step: 5284  Epsilon: 0.948  Avg Reward: 0.46\n",
      "Episode 600/3000  Time Step: 6340  Epsilon: 0.937  Avg Reward: 0.45\n",
      "Episode 700/3000  Time Step: 7322  Epsilon: 0.928  Avg Reward: 0.37\n",
      "Episode 800/3000  Time Step: 8412  Epsilon: 0.917  Avg Reward: 0.47\n",
      "Episode 900/3000  Time Step: 9484  Epsilon: 0.906  Avg Reward: 0.46\n",
      "Episode 1000/3000  Time Step: 10562  Epsilon: 0.895  Avg Reward: 0.46\n",
      "Episode 1100/3000  Time Step: 11558  Epsilon: 0.886  Avg Reward: 0.39\n",
      "Episode 1200/3000  Time Step: 12680  Epsilon: 0.874  Avg Reward: 0.51\n",
      "Episode 1300/3000  Time Step: 14018  Epsilon: 0.861  Avg Reward: 0.70\n",
      "Episode 1400/3000  Time Step: 15216  Epsilon: 0.849  Avg Reward: 0.58\n",
      "Episode 1500/3000  Time Step: 16496  Epsilon: 0.837  Avg Reward: 0.65\n",
      "Episode 1600/3000  Time Step: 17514  Epsilon: 0.827  Avg Reward: 0.41\n",
      "Episode 1700/3000  Time Step: 18682  Epsilon: 0.815  Avg Reward: 0.55\n",
      "Episode 1800/3000  Time Step: 19872  Epsilon: 0.803  Avg Reward: 0.57\n",
      "Episode 1900/3000  Time Step: 21154  Epsilon: 0.791  Avg Reward: 0.66\n",
      "Episode 2000/3000  Time Step: 22482  Epsilon: 0.777  Avg Reward: 0.69\n",
      "Episode 2100/3000  Time Step: 23912  Epsilon: 0.763  Avg Reward: 0.79\n",
      "Episode 2200/3000  Time Step: 25134  Epsilon: 0.751  Avg Reward: 0.60\n",
      "Episode 2300/3000  Time Step: 26514  Epsilon: 0.738  Avg Reward: 0.75\n",
      "Episode 2400/3000  Time Step: 28118  Epsilon: 0.722  Avg Reward: 0.95\n",
      "Episode 2500/3000  Time Step: 29832  Epsilon: 0.705  Avg Reward: 1.06\n",
      "Episode 2600/3000  Time Step: 31374  Epsilon: 0.689  Avg Reward: 0.87\n",
      "Episode 2700/3000  Time Step: 32840  Epsilon: 0.675  Avg Reward: 0.82\n",
      "Episode 2800/3000  Time Step: 34508  Epsilon: 0.658  Avg Reward: 1.01\n",
      "Episode 2900/3000  Time Step: 36046  Epsilon: 0.643  Avg Reward: 0.90\n",
      "Episode 3000/3000  Time Step: 37932  Epsilon: 0.624  Avg Reward: 1.22\n"
     ]
    }
   ],
   "source": [
    "# Train Vanilla DQN\n",
    "\n",
    "print(\"TRAINING VANILLA DQN\")\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "vanilla_agent = DQNAgent(\n",
    "    env, \n",
    "    gamma=DISCOUNT_FACTOR,\n",
    "    lr=LR, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    eps_start=EPS_START,\n",
    "    eps_end=EPS_END,\n",
    "    schedule_duration=SCHEDULE_DURATION,\n",
    "    update_freq=UPDATE_FREQ,\n",
    "    maxlen=REPLAY_BUFFER_SIZE,\n",
    "    use_double_dqn=False,\n",
    ")\n",
    "vanilla_stats = vanilla_agent.train(NUM_EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multi-step DQN\n",
    "print(\"TRAINING MULTISTEP DQN\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "multistep_agent = DQNAgent(\n",
    "    env, \n",
    "    gamma=0.99,\n",
    "    lr=0.00025, \n",
    "    batch_size=32,\n",
    "    eps_start=1.0, \n",
    "    eps_end=0.01,\n",
    "    schedule_duration=100_000,  \n",
    "    update_freq=1000,\n",
    "    maxlen=100_000,\n",
    "    n_steps=10,\n",
    "    use_double_dqn=False\n",
    ")\n",
    "multistep_stats = multistep_agent.train(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Double DQN\n",
    "\n",
    "print(\"TRAINING DOUBLE DQN\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "double_agent = DQNAgent(\n",
    "    env, \n",
    "    gamma=DISCOUNT_FACTOR,\n",
    "    lr=LR, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    eps_start=EPS_START,\n",
    "    eps_end=EPS_END,\n",
    "    schedule_duration=SCHEDULE_DURATION,\n",
    "    update_freq=UPDATE_FREQ,\n",
    "    maxlen=REPLAY_BUFFER_SIZE,\n",
    "    use_double_dqn=True,\n",
    ")\n",
    "double_stats = double_agent.train(NUM_EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dueling extension \n",
    "print(\"TRAINING Dueling extension \")\n",
    "\n",
    "# Choose  environment\n",
    "env = gym.make('MinAtar/Breakout-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "dueling_agent = DQNAgent(\n",
    "    env, \n",
    "    net_type=\"dueling\",\n",
    "    gamma=DISCOUNT_FACTOR,\n",
    "    lr=LR, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    eps_start=EPS_START,\n",
    "    eps_end=EPS_END,\n",
    "    schedule_duration=SCHEDULE_DURATION,\n",
    "    update_freq=UPDATE_FREQ,\n",
    "    maxlen=REPLAY_BUFFER_SIZE,\n",
    "    use_double_dqn=False,\n",
    ")\n",
    "dueling_stats = dueling_agent.train(NUM_EPISODES)\n",
    "\n",
    "# saving stats to resuse for analysis \n",
    "\n",
    "np.savez(\"dueling_stats.npz\",\n",
    "         episode_rewards=dueling_stats.episode_rewards,\n",
    "         episode_lengths=dueling_stats.episode_lengths,\n",
    "         losses=dueling_stats.losses)\n",
    "\n",
    "print(\"Dueling stats saved to dueling_stats.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all three extensions together \n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING Double + Multistep + Dueling extension together\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Choose  environment\n",
    "env = gym.make('MinAtar/Breakout-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "dueling_agent = DQNAgent(\n",
    "    env, \n",
    "    net_type=\"dueling\",\n",
    "    gamma=DISCOUNT_FACTOR,\n",
    "    lr=LR, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    eps_start=EPS_START,\n",
    "    eps_end=EPS_END,\n",
    "    schedule_duration=SCHEDULE_DURATION,\n",
    "    update_freq=UPDATE_FREQ,\n",
    "    maxlen=REPLAY_BUFFER_SIZE,\n",
    "    n_steps=10,\n",
    "    use_double_dqn=True,\n",
    ")\n",
    "overall_stats = dueling_agent.train(NUM_EPISODES)\n",
    "\n",
    "# saving stats to resuse for analysis \n",
    "\n",
    "np.savez(\"results/overall_stats.npz\",\n",
    "         episode_rewards=overall_stats.episode_rewards,\n",
    "         episode_lengths=overall_stats.episode_lengths,\n",
    "         losses=overall_stats.losses)\n",
    "\n",
    "print(\"overall stats saved to results/overall_stats.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IImage\n",
    "from PIL import Image\n",
    "\n",
    "def save_rgb_animation(rgb_arrays, filename, duration=100):\n",
    "    \"\"\"Save an animated GIF from a list of RGB arrays.\"\"\"\n",
    "    frames = []\n",
    "    \n",
    "    for rgb_array in rgb_arrays:\n",
    "        rgb_array = (rgb_array * 255).astype(np.uint8)\n",
    "        rgb_array = rgb_array.repeat(48, axis=0).repeat(48, axis=1)\n",
    "        img = Image.fromarray(rgb_array)\n",
    "        frames.append(img)\n",
    "    \n",
    "    frames[0].save(filename, save_all=True, append_images=frames[1:], duration=duration, loop=0)\n",
    "\n",
    "def rendered_rollout(policy, env, max_steps=1_000):\n",
    "    \"\"\"Rollout for one episode while saving all rendered images.\"\"\"\n",
    "    obs, _ = env.reset()\n",
    "    imgs = [env.render()]\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        action = policy(torch.as_tensor(obs, dtype=torch.float32).unsqueeze(0))\n",
    "        obs, _, terminated, truncated, _ = env.step(action)\n",
    "        imgs.append(env.render())\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    return imgs\n",
    "\n",
    "# Generate videos \n",
    "\n",
    "vanilla_policy = make_epsilon_greedy_policy(vanilla_agent.q, num_actions=env.action_space.n)\n",
    "vanilla_imgs = rendered_rollout(vanilla_policy, env)\n",
    "save_rgb_animation(vanilla_imgs, \"Fvanilla_dqn_trained.gif\")\n",
    "\n",
    "\n",
    "double_policy = make_epsilon_greedy_policy(double_agent.q, num_actions=env.action_space.n)\n",
    "double_imgs = rendered_rollout(double_policy, env)\n",
    "save_rgb_animation(double_imgs, \"Fdouble_dqn_trained.gif\")\n",
    "\n",
    "# Display both\n",
    "print(\"\\nVanilla DQN Gameplay:\")\n",
    "display(IImage(filename=\"Fvanilla_dqn_trained.gif\"))\n",
    "\n",
    "print(\"\\nDouble DQN Gameplay:\")\n",
    "display(IImage(filename=\"Fdouble_dqn_trained.gif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "\n",
    "# EVALUATION: Test Trained Agents\n",
    "\n",
    "def evaluate_agent(agent, env, num_episodes=100, seed=42):\n",
    "    \"\"\"\n",
    "    Evaluate a trained agent over multiple episodes.\n",
    "    Returns: list of episode rewards\n",
    "    \"\"\"\n",
    "    policy = make_epsilon_greedy_policy(agent.q, num_actions=env.action_space.n)\n",
    "    rewards = []\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        obs, _ = env.reset(seed=seed + i)  # Reproducible episodes\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(1000):  # Max steps per episode\n",
    "            # Greedy policy (no exploration during evaluation)\n",
    "            action = policy(torch.as_tensor(obs, dtype=torch.float32).unsqueeze(0), epsilon=0.0)\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break   \n",
    "        rewards.append(total_reward)\n",
    "    return rewards\n",
    "\n",
    "# PLOTTING FUNCTIONS \n",
    "\n",
    "def smooth_data(data: np.ndarray, window: int) -> pd.Series:\n",
    "    # smoothen data using rolling mean\n",
    "    return pd.Series(data).rolling(window, min_periods=window).mean()\n",
    "\n",
    "\n",
    "def plot_episode_rewards(ax: plt.Axes, agents_stats: Dict[str, any], smoothing_window: int = 20):\n",
    "    # Plot smoothed episode rewards for all extensions\n",
    "\n",
    "    \n",
    "    for i, (name, stats) in enumerate(agents_stats.items()):\n",
    "        smoothed = smooth_data(stats.episode_rewards, smoothing_window)\n",
    "        ax.plot(smoothed, label=name, linewidth=2, alpha=0.9)\n",
    "    \n",
    "    ax.set_xlabel('Episode', fontsize=12)\n",
    "    ax.set_ylabel('Episode Reward (Smoothed)', fontsize=12)\n",
    "    ax.set_title(f'Episode Rewards Over Time (smoothed over {smoothing_window} episodes)', fontsize=12)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "def plot_moving_average_rewards(ax: plt.Axes, agents_stats: Dict[str, any], window: int = 100):\n",
    "\n",
    "    # Plot moving average of episode rewards.\n",
    "    for i, (name, stats) in enumerate(agents_stats.items()):\n",
    "        cumulative = pd.Series(stats.episode_rewards).rolling(window, min_periods=1).mean()\n",
    "        ax.plot(cumulative, label=name, linewidth=2, alpha=0.9)\n",
    "    \n",
    "    ax.set_xlabel('Episode', fontsize=12)\n",
    "    ax.set_ylabel(f'Average Reward (Last {window} episodes)', fontsize=12)\n",
    "    ax.set_title('Moving Average of Episode Rewards', fontsize=12)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "def create_comparison_plot(\n",
    "    agents_stats: Dict[str, any],\n",
    "    title: str = \"DQN Variants Comparison\",\n",
    "    smoothing_window: int = 20,\n",
    "    loss_smoothing: int = 100,\n",
    "    moving_avg_window: int = 100,\n",
    "    save_path: str = None,\n",
    "    figsize: Tuple[int, int] = (15, 10)\n",
    "):\n",
    "    \"\"\"\n",
    "    Comparison plot for multiple extensions\n",
    "    \n",
    "    Example usage:\n",
    "        agents = {\n",
    "            \"Vanilla DQN\": vanilla_stats,\n",
    "            \"Double DQN\": double_stats,\n",
    "            \"Dueling DQN\": dueling_stats\n",
    "        }\n",
    "        create_comparison_plot(agents, title=\"DQN Extensions Comparison\")\n",
    "    \"\"\"\n",
    "\n",
    "    # Create figure and subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Episode Rewards\n",
    "    plot_episode_rewards(axes[0], agents_stats, smoothing_window)\n",
    "    \n",
    "    # Plot 2: Moving Average Rewards\n",
    "    plot_moving_average_rewards(axes[1], agents_stats, moving_avg_window)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_performance_summary(agents_stats: Dict[str, any], test_results: Dict[str, List[float]] = None, last_n_episodes: int = 100):\n",
    "    # Print a comprehensive performance summary table.\n",
    "\n",
    "    print(\"PERFORMANCE SUMMARY\")\n",
    "\n",
    "    # Training Performance\n",
    "    print(f\"\\nTraining Performance (Last {last_n_episodes} Episodes):\")\n",
    "    print(f\"{'Agent':<30} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10}\")\n",
    "    for name, stats in agents_stats.items():\n",
    "        last_rewards = stats.episode_rewards[-last_n_episodes:]\n",
    "        print(f\"{name:<30} {np.mean(last_rewards):>10.2f} {np.std(last_rewards):>10.2f} \"\n",
    "              f\"{np.min(last_rewards):>10.2f} {np.max(last_rewards):>10.2f}\")\n",
    "    \n",
    "    # Test Performance \n",
    "    if test_results:\n",
    "        print(f\"\\n\\nTest Performance:\")\n",
    "        print(f\"{'Agent':<30} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10}\")\n",
    "        baseline_mean = None\n",
    "        for name, rewards in test_results.items():\n",
    "            mean_reward = np.mean(rewards)\n",
    "            if baseline_mean is None:\n",
    "                baseline_mean = mean_reward\n",
    "            \n",
    "            improvement = ((mean_reward - baseline_mean) / baseline_mean * 100) if baseline_mean > 0 else 0\n",
    "            \n",
    "            print(f\"{name:<30} {mean_reward:>10.2f} {np.std(rewards):>10.2f} \"\n",
    "                  f\"{np.min(rewards):>10.2f} {np.max(rewards):>10.2f}\", end=\"\")\n",
    "            \n",
    "            if name != list(test_results.keys())[0]: \n",
    "                print(f\"  ({improvement:+.1f}%)\", end=\"\")\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training your extensions\n",
    "agents_stats = {\n",
    "    \"Vanilla DQN\": vanilla_stats,\n",
    "    \"Double DQN\": double_stats,\n",
    "    \"Multi-step DQN\": multistep_stats,\n",
    "    #\"Dueling DQN\": dueling_stats,\n",
    "}\n",
    "\n",
    "# creates both the plots and saves to file\n",
    "create_comparison_plot(\n",
    "    agents_stats,\n",
    "    title=\"Comparison of DQN Extensions against Vanilla DQN\",\n",
    "    save_path=\"results/comparison.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the extensions\n",
    "vanilla_test_rewards = evaluate_agent(vanilla_agent, env, 100, SEED)\n",
    "double_test_rewards = evaluate_agent(double_agent, env, 100, SEED)\n",
    "multistep_test_rewards = evaluate_agent(multistep_agent, env, 100, SEED)\n",
    "#dueling_test_rewards = evaluate_agent(dueling_agent, env, 100, SEED)\n",
    "\n",
    "test_results = {\n",
    "    \"Vanilla DQN\": vanilla_test_rewards,\n",
    "    \"Double DQN\": double_test_rewards,\n",
    "    \"Multi-step DQN\": multistep_test_rewards,\n",
    "    #\"Dueling DQN\": dueling_test_rewards\n",
    "}\n",
    "\n",
    "print_performance_summary(agents_stats, test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOME DRAFTS IDK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what the agent learned\n",
    "print(\"=== Double DQN Agent Analysis ===\\n\")\n",
    "\n",
    "# Check final epsilon value\n",
    "print(f\"Final epsilon reached: {linear_epsilon_decay(EPS_START, EPS_END, SCHEDULE_DURATION, SCHEDULE_DURATION):.4f}\")\n",
    "\n",
    "# Analyze Q-values\n",
    "with torch.no_grad():\n",
    "    # Sample some states from replay buffer (unpack the result - first element are the states)\n",
    "    if len(double_agent.buffer) > 0:\n",
    "        sample_states, _, _, _, _, _ = double_agent.buffer.sample(min(32, len(double_agent.buffer)))\n",
    "        q_values = double_agent.q(sample_states.float())\n",
    "        \n",
    "        print(f\"\\nQ-value statistics from sampled states:\")\n",
    "        print(f\"  Mean Q-value: {q_values.mean().item():.4f}\")\n",
    "        print(f\"  Std Q-value: {q_values.std().item():.4f}\")\n",
    "        print(f\"  Max Q-value: {q_values.max().item():.4f}\")\n",
    "        print(f\"  Min Q-value: {q_values.min().item():.4f}\")\n",
    "        print(f\"  Q-value range per state (avg): {(q_values.max(dim=1)[0] - q_values.min(dim=1)[0]).mean().item():.4f}\")\n",
    "\n",
    "# Check action distribution in replay buffer\n",
    "if len(double_agent.buffer) > 0:\n",
    "    _, actions, _, _, _, _ = double_agent.buffer.sample(min(1000, len(double_agent.buffer)))\n",
    "    actions = actions.long()  # ensure integer dtype for bincount\n",
    "    action_counts = torch.bincount(actions, minlength=env.action_space.n)\n",
    "    print(f\"\\nAction distribution in replay buffer:\")\n",
    "    for i, count in enumerate(action_counts):\n",
    "        print(f\"  Action {i}: {count.item()} ({count.item()/len(actions)*100:.1f}%)\")\n",
    "\n",
    "# Compare with vanilla DQN\n",
    "print(\"\\n=== Vanilla DQN Agent Analysis ===\\n\")\n",
    "with torch.no_grad():\n",
    "    if len(vanilla_agent.buffer) > 0:\n",
    "        sample_states, _, _, _, _, _ = vanilla_agent.buffer.sample(min(32, len(vanilla_agent.buffer)))\n",
    "        q_values = vanilla_agent.q(sample_states.float())\n",
    "        \n",
    "        print(f\"Q-value statistics from sampled states:\")\n",
    "        print(f\"  Mean Q-value: {q_values.mean().item():.4f}\")\n",
    "        print(f\"  Std Q-value: {q_values.std().item():.4f}\")\n",
    "        print(f\"  Max Q-value: {q_values.max().item():.4f}\")\n",
    "        print(f\"  Min Q-value: {q_values.min().item():.4f}\")\n",
    "        print(f\"  Q-value range per state (avg): {(q_values.max(dim=1)[0] - q_values.min(dim=1)[0]).mean().item():.4f}\")\n",
    "\n",
    "# Check action distribution in replay buffer\n",
    "if len(vanilla_agent.buffer) > 0:\n",
    "    _, actions, _, _, _, _ = vanilla_agent.buffer.sample(min(1000, len(vanilla_agent.buffer)))\n",
    "    actions = actions.long()  # ensure integer dtype for bincount\n",
    "    action_counts = torch.bincount(actions, minlength=env.action_space.n)\n",
    "    print(f\"\\nAction distribution in replay buffer:\")\n",
    "    for i, count in enumerate(action_counts):\n",
    "        print(f\"  Action {i}: {count.item()} ({count.item()/len(actions)*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
